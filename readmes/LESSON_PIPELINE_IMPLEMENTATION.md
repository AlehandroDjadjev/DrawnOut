# ğŸ¯ Lesson Pipeline Implementation - In Progress

## Overview

Building an end-to-end lesson generation system that:
1. **Researches images** from multiple sources in parallel
2. **Generates lesson script** with intelligent `[IMAGE ...]` tags
3. **Embeds images** using SigLIP and stores in Pinecone vector database
4. **Matches images** semantically to script tags via vector similarity
5. **Transforms images** using image-to-image models (ComfyUI)
6. **Assembles final lesson** with contextually relevant, customized visuals

---

## Architecture

```
User Prompt: "Explain DNA structure"
         â”‚
         â–¼
    /api/generate-lesson
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                      â”‚                      â”‚
         â–¼                      â–¼                      â–¼
   IMAGE RESEARCH        SCRIPT GENERATION    (parallel)
   â”œâ”€ Search sources     â”œâ”€ GPT-4 generates
   â”œâ”€ Find 40 images     â”œâ”€ Adds [IMAGE...] tags
   â”œâ”€ Embed with SigLIP  â””â”€ Returns script draft
   â””â”€ Store in Pinecone
         â”‚                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            PARSE IMAGE TAGS
            [[IMAGE:img_1]]
                    â”‚
                    â–¼
         SEMANTIC IMAGE MATCHING
         â”œâ”€ Embed tag prompts
         â”œâ”€ Query Pinecone
         â””â”€ Get best base images
                    â”‚
                    â–¼
         IMAGE-TO-IMAGE TRANSFORM
         â”œâ”€ Take base image
         â”œâ”€ Apply tag prompt
         â””â”€ Generate final image
                    â”‚
                    â–¼
            INJECT INTO SCRIPT
            Return complete lesson
```

---

## âœ… Completed Components

### 1. Core Types (`lesson_pipeline/types.py`)
- âœ… `UserPrompt` - Input prompt
- âœ… `ImageCandidate` - Researched images
- âœ… `ImageEmbeddingRecord` - Vector database records
- âœ… `ImageTag` - Parsed IMAGE tags
- âœ… `ScriptDraft` - Raw script with tags
- âœ… `ResolvedImage` - Final images with metadata
- âœ… `LessonDocument` - Complete lesson output

### 2. Configuration (`lesson_pipeline/config.py`)
- âœ… Environment-based config
- âœ… Pinecone settings (API key, index name, environment)
- âœ… SigLIP model configuration
- âœ… ComfyUI server URL
- âœ… Timeouts, retries, defaults
- âœ… Loaded from environment variables

### 3. IMAGE Tag Parser (`lesson_pipeline/utils/image_tags.py`)
- âœ… Regex-based tag parsing
- âœ… Flexible attribute parsing (any order)
- âœ… Placeholder injection `[[IMAGE:id]]`
- âœ… Image injection with markdown format
- âœ… Validation for tags
- âœ… Support for: `id`, `prompt`, `style`, `aspect`, `size`, `strength`, `guidance`

**Example:**
```
[IMAGE id="img_1" prompt="DNA double helix" style="scientific diagram" aspect="16:9"]
â†’ Parsed to ImageTag object
â†’ Replaced with [[IMAGE:img_1]]
â†’ Later injected as: ![DNA double helix](https://...){.lesson-image}
```

### 4. SigLIP Embedding Service (`lesson_pipeline/services/embeddings.py`)
- âœ… Text embedding via SigLIP
- âœ… Image embedding via SigLIP  
- âœ… Batch processing for efficiency
- âœ… GPU support (CUDA)
- âœ… Lazy model loading
- âœ… Error handling
- âœ… Singleton pattern

**Features:**
- Model: `google/siglip-so400m-patch14-384`
- Output: 1152-dimensional vectors
- Batched inference (8 images at a time)
- Normalized embeddings for cosine similarity

### 5. Pinecone Vector Store (`lesson_pipeline/services/vector_store.py`)
- âœ… Index management (auto-create)
- âœ… Batch upsert (100 vectors at a time)
- âœ… Semantic search with filters
- âœ… Topic-based filtering
- âœ… Metadata storage
- âœ… Stats API
- âœ… Delete by topic

**Features:**
- Index: `lesson-images`
- Metric: Cosine similarity
- Serverless (AWS)
- Automatic batching

---

## ğŸ”„ In Progress Components

### 6. Image Research Integration (NEXT)
File: `lesson_pipeline/services/image_researcher.py`

**Needed:**
- Wrapper around existing `image_researcher` app
- Interface: `research_images(prompt: str, subject: str, max_images: int) -> List[ImageCandidate]`
- Convert image_researcher results to `ImageCandidate` format

### 7. Script Writer Service (NEXT)
File: `lesson_pipeline/services/script_writer.py`

**Needed:**
- Wrapper around existing timeline generator or new GPT-4 service
- System prompt that instructs LLM to use `[IMAGE ...]` tags
- Interface: `generate_script(prompt: UserPrompt) -> ScriptDraft`

**System Prompt Example:**
```
You are an educational content writer. Generate a lesson script with embedded image tags.

For each visual concept, add: [IMAGE id="unique_id" prompt="descriptive prompt" style="photo|diagram|illustration" aspect="16:9"]

Example:
The neuron is the basic building block...
[IMAGE id="img_1" prompt="labeled diagram of a neuron showing dendrites, cell body, axon" style="scientific diagram" aspect="16:9"]
```

### 8. Image-to-Image Service (NEXT)
File: `lesson_pipeline/services/image_to_image.py`

**Needed:**
- Integration with ComfyUI (existing `imggen` app)
- Interface: `transform_image(base_url: str, prompt: str, params: dict) -> str`
- Handle style transfer, aspect ratio adjustment
- Return final image URL

### 9. Pipeline Orchestration (NEXT)
File: `lesson_pipeline/pipelines/orchestrator.py`

**Needed:**
- `generate_lesson(prompt: str, subject: str) -> LessonDocument`
- Coordinate all steps:
  1. Parallel: research images + generate script
  2. Parse IMAGE tags
  3. Resolve tags to base images (Pinecone query)
  4. Transform images (img2img)
  5. Inject into script
  6. Return LessonDocument

### 10. API Endpoint (NEXT)
File: `lesson_pipeline/views.py`

**Needed:**
- `POST /api/lesson-pipeline/generate/`
- Request: `{ "prompt": "...", "subject": "..." }`
- Response: `LessonDocument` JSON
- Error handling
- Logging

---

## Dependencies

### Python Packages Needed
```bash
# Already installed:
- transformers
- tokenizers
- torch
- Pillow
- requests

# Need to add:
pip install pinecone-client
```

### Environment Variables
```bash
# .env file
PINECONE_API_KEY=your_api_key
PINECONE_ENVIRONMENT=us-east-1-aws
PINECONE_INDEX_NAME=lesson-images

SIGLIP_MODEL_NAME=google/siglip-so400m-patch14-384
EMBEDDING_DIMENSION=1152

COMFY_SERVER_URL=http://127.0.0.1:8188

MAX_IMAGES_PER_PROMPT=40
DEFAULT_ASPECT_RATIO=16:9
DEFAULT_SIZE=1024x576
```

---

## File Structure

```
backend/lesson_pipeline/
â”œâ”€â”€ __init__.py                    âœ… Package init
â”œâ”€â”€ apps.py                        âœ… Django app config
â”œâ”€â”€ types.py                       âœ… Shared data types
â”œâ”€â”€ config.py                      âœ… Configuration
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py                âœ…
â”‚   â””â”€â”€ image_tags.py              âœ… IMAGE tag parser
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py                ğŸ”„ TODO
â”‚   â”œâ”€â”€ embeddings.py              âœ… SigLIP service
â”‚   â”œâ”€â”€ vector_store.py            âœ… Pinecone service
â”‚   â”œâ”€â”€ image_researcher.py        ğŸ”„ TODO (wrapper)
â”‚   â”œâ”€â”€ script_writer.py           ğŸ”„ TODO (wrapper/new)
â”‚   â””â”€â”€ image_to_image.py          ğŸ”„ TODO (ComfyUI wrapper)
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py                ğŸ”„ TODO
â”‚   â”œâ”€â”€ image_ingestion.py         ğŸ”„ TODO (research â†’ Pinecone)
â”‚   â”œâ”€â”€ image_resolver.py          ğŸ”„ TODO (tags â†’ base images)
â”‚   â”œâ”€â”€ image_transformation.py    ğŸ”„ TODO (img2img batch)
â”‚   â””â”€â”€ orchestrator.py            ğŸ”„ TODO (main pipeline)
â”‚
â”œâ”€â”€ views.py                       ğŸ”„ TODO (API endpoint)
â”œâ”€â”€ urls.py                        ğŸ”„ TODO (URL routing)
â””â”€â”€ README.md                      ğŸ”„ TODO (documentation)
```

---

## Next Steps

### Immediate (Phase 2):
1. âœ… Create `services/__init__.py`
2. âœ… Create `services/image_researcher.py` wrapper
3. âœ… Create `services/script_writer.py` with IMAGE tag instructions
4. âœ… Create `services/image_to_image.py` ComfyUI wrapper

### Phase 3:
5. Create `pipelines/image_ingestion.py`
6. Create `pipelines/image_resolver.py`
7. Create `pipelines/image_transformation.py`
8. Create `pipelines/orchestrator.py`

### Phase 4:
9. Create API endpoint in `views.py`
10. Add URL routing in `urls.py`
11. Register app in Django settings
12. Install `pinecone-client`
13. Create test endpoint

### Phase 5:
14. Integration testing
15. Error handling improvements
16. Logging enhancements
17. Performance optimization
18. Documentation

---

## Testing Plan

### Unit Tests
- âœ… IMAGE tag parser
- â³ Embedding service (mock model)
- â³ Vector store (mock Pinecone)
- â³ Each pipeline component

### Integration Tests
- â³ Full pipeline with real data
- â³ Error scenarios
- â³ Parallel execution
- â³ Timeout handling

### End-to-End Test
```bash
curl -X POST http://localhost:8000/api/lesson-pipeline/generate/ \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain DNA structure and replication",
    "subject": "Biology"
  }'
```

Expected output:
- Complete lesson script with images injected
- 5-10 contextually relevant images
- Each image transformed to match style/requirements
- Total time: 30-90 seconds

---

## Current Status

**Phase 1: COMPLETE** âœ…
- Core types defined
- Configuration system
- IMAGE tag parser
- SigLIP embedding service
- Pinecone vector store

**Phase 2: IN PROGRESS** ğŸ”„
- Image researcher wrapper
- Script writer service
- Image-to-image service

**Estimated Completion:** ~4-6 more hours of development

---

## Design Decisions

### Why SigLIP?
- State-of-the-art vision-language model
- Better than CLIP for text-image matching
- 1152-dim embeddings balance quality/speed
- Native multimodal understanding

### Why Pinecone?
- Managed vector database (no ops)
- Fast similarity search at scale
- Metadata filtering
- Serverless scaling

### Why Image-to-Image?
- Base images provide structure/context
- Transformation allows style consistency
- Better quality than pure text-to-image
- Faster generation

### Why Parallel Research + Script?
- Saves ~30-60 seconds
- Images and script don't depend on each other
- Better user experience
- Efficient resource usage

---

**Status:** ğŸŸ¡ **40% Complete** - Core infrastructure done, pipeline integration in progress


